{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetal Health Classification using AI/ML E-13"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing inital libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "#library for data preprocessing & to read CSV files\n",
    "import pandas as pd\n",
    "\n",
    "# used for linear algebra and multi-dimensional matrix manipulation\n",
    "import numpy as np \n",
    "\n",
    "import os\n",
    "\n",
    "#used for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#matplotlib is used internally, easier to use\n",
    "import seaborn as sb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and viewing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"./fetal_health.csv\")\n",
    "data_frame.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the data\n",
    "\n",
    "This dataset records of features extracted from Cardiotocogram (**CTG**) exams, which were then classified by expert *obstetrician* into 3 classes: \n",
    "- **Normal**\n",
    "- **Suspect**\n",
    "- **Pathological**\n",
    "---\n",
    "The Dataset has the following features/attributes:\n",
    "\n",
    "- **baseline value**: Baseline Fetal Heart Rate (FHR) (beats per minute)\n",
    "- **accelerations**: Number of accelerations per second\n",
    "- **fetal_movement**: Number of fetal movements per second\n",
    "- **uterine_contractions**: Number of uterine contractions per second\n",
    "- **light_decelerations**: Number of light decelerations (LDs) per second\n",
    "- **severe_decelerations**: Number of severe decelerations (SDs) per second\n",
    "- **prolongued_decelerations**: Number of prolonged decelerations (PDs) per second\n",
    "- **abnormal_short_term_variability**: Percentage of time with abnormal short term variability\n",
    "- **mean_value_of_short_term_variability**: Mean value of short term variability\n",
    "- **percentage_of_time_with_abnormal_long_term_variability**: Percentage of time with abnormal long term variability\n",
    "- **mean_value_of_long_term_variability**: Mean value of long term variability\n",
    "- **histogram_width**: Width of histogram made using all values from a record\n",
    "- **histogram_min**: Histogram minimum value\n",
    "- **histogram_max**: Histogram maximum value\n",
    "- **histogram_number_of_peaks**: Number of peaks in the exam histogram\n",
    "- **histogram_number_of_zeroes**: Number of zeros in the exam histogram\n",
    "- **histogram_mode**: Histogram mode\n",
    "- **histogram_mean**: Histogram mean\n",
    "- **histogram_median**: Histogram median\n",
    "- **histogram_variance**: Histogram variance\n",
    "- **histogram_tendency**: Histogram tendency\n",
    "- **fetal_health**: Encoded as 1-Normal; 2-Suspect; 3-Pathological."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreiving information of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA ANALYISIS & PRE-PROCESSING "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical trends in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating if our data is imbalanaced or not by analysing the target output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours=[\"green\",\"gold\", \"red\"]\n",
    "sb.countplot(data=data_frame, x=\"fetal_health\",palette=colours)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see there is an imbalance in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Heatmap to analyze how related any two datapoints are (Co-relation Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = data_frame.corr()\n",
    "plt.figure(figsize=(15,15))\n",
    "sb.heatmap(correlation_matrix, annot=True,center=0)\n",
    "plt.savefig(\"co-relation.svg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the co-relation matrix we can see that\n",
    "1. prolonged_decelerations\n",
    "2. percentage of time with abnormal long term variabilty\n",
    "3. mean value of long term variability\n",
    "\n",
    "have higher co-relation with fetal health and hence these are the most important features that wil be essential in the prediction of the state of the fetus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['baseline value', 'accelerations', 'fetal_movement',\n",
    "       'uterine_contractions', 'light_decelerations', 'severe_decelerations',\n",
    "       'prolongued_decelerations', 'abnormal_short_term_variability',\n",
    "       'mean_value_of_short_term_variability',\n",
    "       'percentage_of_time_with_abnormal_long_term_variability',\n",
    "       'mean_value_of_long_term_variability']\n",
    "for i in cols:\n",
    "#     sb.swarmplot(x=data_frame[\"fetal_health\"], y=data_frame[i], color=\"black\", alpha=0.5,size=1.5)\n",
    "#     sb.boxenplot(x=data_frame[\"fetal_health\"], y=data_frame[i], palette=colours)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the target data from the dataset to visualise the the distribution of the attributtes\n",
    "XData =data_frame.drop([\"fetal_health\"], axis=1)\n",
    "# store the target data for future reference\n",
    "YData = data_frame[\"fetal_health\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the distribution of data BEFORE feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=[\"#483D8B\",\"#4682B4\", \"#87CEFA\"]\n",
    "features=['baseline value', 'accelerations', 'fetal_movement','uterine_contractions', 'light_decelerations', 'severe_decelerations',\n",
    "           'prolongued_decelerations', 'abnormal_short_term_variability', 'mean_value_of_short_term_variability',\n",
    "             'percentage_of_time_with_abnormal_long_term_variability', 'mean_value_of_long_term_variability']\n",
    "plt.figure(figsize=(20,10))\n",
    "sb.boxenplot(data = XData,palette = colors)\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "plt.savefig(\"before-feature-scaling.svg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the distribution of data AFTER feature scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the standard scaler from the scikit-learn library to scale the data/features to make the data suitable for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "attributes = list(XData.columns)\n",
    "s_scaler = StandardScaler()\n",
    "XData_scaled= s_scaler.fit_transform(XData)\n",
    "XData_scaled = pd.DataFrame(XData_scaled, columns=attributes)   \n",
    "XData_scaled.describe().T\n",
    "# Data after feature scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sb.boxenplot(data = XData_scaled,palette = colors)\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "plt.savefig(\"after-feature-scaling.svg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points\n",
    "1. We can see from the above plot that the data is in the same range after the feature scaling\n",
    "2. We can see some outliers in some attributes after the feature scaling\n",
    "3. These outliers are not data entry errors or measurement errors as this data has been extracted from **CTG** data\n",
    "4. Hence the outliers cannot be eliminated or dropped of from the model training as this will result in loss of information, and overfitting of the model (*fits exactly against training data.*)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building (YTBD)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test datasets (yet to be done) - First Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "XDataTrain, XDataTest, YTrain,YTest = train_test_split(XData_scaled,YData,test_size=0.2,random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Training Linear Regression Classifier Model\n",
    "\n",
    "LinearRegressionModel = Pipeline([('lr_classifier',LogisticRegression(random_state=25))])\n",
    "LinearRegressionModel.fit(XDataTrain.values, YTrain.values)\n",
    "filename = \"./major/predict/LinearRegressionModel.sav\"\n",
    "joblib.dump(LinearRegressionModel,filename)\n",
    "cv_score = cross_val_score(LinearRegressionModel, XDataTrain,YTrain, cv=12)\n",
    "print(f\"LinearRegression - {cv_score.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pred = LinearRegressionModel.predict(XDataTest)\n",
    "print(classification_report(YTest,linear_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifierModel = Pipeline([('dt_classifier',DecisionTreeClassifier(random_state=25))])\n",
    "DecisionTreeClassifierModel.fit(XDataTrain.values, YTrain)\n",
    "filename = \"./major/predict/DecisionTreeClassifierModel.sav\"\n",
    "joblib.dump(DecisionTreeClassifierModel,filename)\n",
    "cv_score = cross_val_score(DecisionTreeClassifierModel, XDataTrain,YTrain, cv=12)\n",
    "print(f\"DecisionTreeClassifier - {cv_score.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree_pred = DecisionTreeClassifierModel.predict(XDataTest)\n",
    "print(classification_report(YTest,decisionTree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifierModel = Pipeline([('rf_classifier',RandomForestClassifier())])\n",
    "RandomForestClassifierModel.fit(XDataTrain, YTrain)\n",
    "filename = \"./major/predict/RandomForestClassifierModel.sav\"\n",
    "joblib.dump(RandomForestClassifierModel,filename)\n",
    "cv_score = cross_val_score(RandomForestClassifierModel, XDataTrain,YTrain, cv=12)\n",
    "print(f\"RandomForestClassifier - {cv_score.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest_pred = RandomForestClassifierModel.predict(XDataTest)\n",
    "print(classification_report(YTest,randomForest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCModel = Pipeline([('sv_classifier',SVC())])\n",
    "SVCModel.fit(XDataTrain, YTrain)\n",
    "filename = \"./major/predict/SVCModel.sav\"\n",
    "joblib.dump(SVCModel,filename)\n",
    "cv_score = cross_val_score(SVCModel, XDataTrain,YTrain, cv=12)\n",
    "print(f\"SVC - {cv_score.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pred = SVCModel.predict(XDataTest)\n",
    "print(classification_report(YTest,svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GradientBoostingClassifierModel = Pipeline([('gbcl_classifier',GradientBoostingClassifier())])\n",
    "GradientBoostingClassifierModel.fit(XDataTrain, YTrain)\n",
    "filename = \"./major/predict/GradientBoostingClassifierModel.sav\"\n",
    "joblib.dump(GradientBoostingClassifierModel,filename)\n",
    "cv_score = cross_val_score(GradientBoostingClassifierModel, XDataTrain,YTrain, cv=12)\n",
    "print(f\"GradientBoostingClassifierModel - {cv_score.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_pred = GradientBoostingClassifierModel.predict(XDataTest)\n",
    "print(classification_report(YTest,gradient_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNeighborsClassifierModel = Pipeline([('knn_classifier',KNeighborsClassifier())])\n",
    "KNeighborsClassifierModel.fit(XDataTrain, YTrain)\n",
    "filename = \"./major/predict/KNeighborsClassifierModel.sav\"\n",
    "joblib.dump(KNeighborsClassifierModel,filename)\n",
    "cv_score = cross_val_score(KNeighborsClassifierModel, XDataTrain,YTrain, cv=12)\n",
    "print(f\"KNeighborsClassifierModel - {cv_score.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = KNeighborsClassifierModel.predict(XDataTest)\n",
    "print(classification_report(YTest,knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tune the model we are choosing the hyperparameter values \n",
    "# random forest hypertune\n",
    "# parameters = { \n",
    "#     'n_estimators': [100,150, 200,500,700,900],\n",
    "#     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#     'max_depth' : [4,6,8,12,14,16],\n",
    "#     'criterion' :['gini', 'entropy'],\n",
    "#     'n_jobs':[-1,1,None]\n",
    "# }\n",
    "# RandomForestClassifierModelHyperTuned = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameters, cv= 5)\n",
    "# RandomForestClassifierModelHyperTuned.fit(XDataTrain, YTrain)\n",
    "# filename = \"./major/predict/RandomForestClassifierModelHyperTuned.sav\"\n",
    "# joblib.dump(RandomForestClassifierModelHyperTuned,filename)\n",
    "# PredGBCLModelTrain = RandomForestClassifierModelHyperTuned.predict(XDataTest)\n",
    "\n",
    "# gradient boost hyperparams\n",
    "# params = {\"loss\": [\"deviance\"],\n",
    "#               \"learning_rate\": [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1], \n",
    "#               \"n_estimators\": [200, 350, 500, 750],\n",
    "#               \"max_depth\": [3, 6, 8]\n",
    "#               }\n",
    "# params = {\"loss\": [\"log_loss\"],\n",
    "#               \"learning_rate\": [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1], \n",
    "#               \"n_estimators\": [200, 350, 500, 750],\n",
    "#               \"max_depth\": [3, 6, 8]\n",
    "#               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFCLModelTrain = RandomForestClassifierModelHyperTuned.score(XDataTrain,YTrain)\n",
    "# RFCLModelTest = RandomForestClassifierModelHyperTuned.score(XDataTest,YTest)\n",
    "\n",
    "# print(f\"r^2(coeff of determination) on train set = {round(RFCLModelTrain, 3)}\")\n",
    "# print(f\"r^2(coeff of determination) on test set = {round(RFCLModelTest, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBCLModelTrain = GradientBoostingClassifierModel.score(XDataTrain,YTrain)\n",
    "GBCLModelTest = GradientBoostingClassifierModel.score(XDataTest,YTest)\n",
    "cv_score = cross_val_score(GradientBoostingClassifierModel, XDataTrain,YTrain, cv=12)\n",
    "print(f\"GradientBoostingClassifierModel - {cv_score.mean()}\")\n",
    "\n",
    "print(f\"r^2(coeff of determination) on train set = {round(GBCLModelTrain, 3)}\")\n",
    "print(f\"r^2(coeff of determination) on test set = {round(GBCLModelTest, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifierModelHyperTuned\n",
    "# best_params = RandomForestClassifierModelHyperTuned.best_params_ # {'criterion': 'entropy', 'max_depth': 14, 'max_features': 'auto', 'n_estimators': 100, 'n_jobs': None}\n",
    "# print(best_params)\n",
    "\n",
    "# RF_model = RandomForestClassifier(**RandomForestClassifierModelHyperTuned.best_params_)\n",
    "RF_model = RandomForestClassifier(n_estimators= 100,criterion='entropy', max_depth=14, max_features= 'auto',  n_jobs=None)\n",
    "RF_model.fit(XDataTrain, YTrain)\n",
    "\n",
    "cv_score = cross_val_score(RF_model, XDataTrain,YTrain, cv=12)\n",
    "print(f\"RandomForestClassifierModelHyperTuned - {cv_score.mean()}\")\n",
    "RFCLModelTrain = RF_model.score(XDataTrain,YTrain)\n",
    "RFCLModelTest = RF_model.score(XDataTest,YTest)\n",
    "filename = \"./major/predict/RandomForestClassifierModelHyperTuned.sav\"\n",
    "joblib.dump(RF_model,filename)\n",
    "\n",
    "print(f\"r^2(coeff of determination) on train set = {round(RFCLModelTrain, 3)}\")\n",
    "print(f\"r^2(coeff of determination) on test set = {round(RFCLModelTest, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gbcl = GradientBoostingClassifierModel.predict(XDataTest)\n",
    "acc = accuracy_score(YTest,pred_gbcl)\n",
    "print(f\" Testing Score of the model is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gbcl = RF_model.predict(XDataTest)\n",
    "acc = accuracy_score(YTest,pred_gbcl)\n",
    "print(f\" Testing Score of the model is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(YTest, pred_gbcl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.subplots(figsize=(12,8))\n",
    "cf_matrix = confusion_matrix(YTest, pred_gbcl)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), cmap=sns.diverging_palette(250, 10, s=80, l=55, n=9, as_cmap=True),annot = True, annot_kws = {'size':20})\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e8f45abd753db01617723dd79533d0c7f3d680204f3fc966b6ce49d276012f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
